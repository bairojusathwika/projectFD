{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Dv7uRzYRx5yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d8cf8c-4d5d-471f-fdff-3f9e78ca6b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinstall with specific version\n",
        "!pip install --upgrade qdrant-client==1.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T12lizNUoCt1",
        "outputId": "de4d8f3c-b249-4a9f-8899-f4c448ab7f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qdrant-client==1.7.0\n",
            "  Downloading qdrant_client-1.7.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.7.0) (1.76.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client==1.7.0)\n",
            "  Downloading grpcio_tools-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: httpx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.14.0->qdrant-client==1.7.0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.7.0) (2.0.2)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client==1.7.0)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.7.0) (2.12.3)\n",
            "Collecting urllib3<2.0.0,>=1.26.14 (from qdrant-client==1.7.0)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant-client==1.7.0) (4.15.0)\n",
            "Collecting protobuf<7.0.0,>=6.31.1 (from grpcio-tools>=1.41.0->qdrant-client==1.7.0)\n",
            "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from grpcio-tools>=1.41.0->qdrant-client==1.7.0) (75.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.14.0->qdrant-client==1.7.0) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->qdrant-client==1.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->qdrant-client==1.7.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->qdrant-client==1.7.0) (0.4.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client==1.7.0) (4.1.0)\n",
            "Downloading qdrant_client-1.7.0-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, protobuf, portalocker, grpcio-tools, qdrant-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-tools-1.76.0 portalocker-2.10.1 protobuf-6.33.5 qdrant-client-1.7.0 urllib3-1.26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "a6d51081a4be47ddb6ad828f5a51b6c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FRAUD DETECTION: XGBoost + Qdrant Hybrid Model with ONNX Export\n",
        "\n",
        "Installation:\n",
        "    pip install qdrant-client==1.7.0 xgboost pandas numpy scikit-learn onnx onnxruntime skl2onnx\n",
        "\n",
        "    # Note: Use specific Qdrant version for compatibility\n",
        "\n",
        "Qdrant Setup:\n",
        "    # Option 1: Docker (recommended for production)\n",
        "    docker run -p 6333:6333 -p 6334:6334 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\n",
        "\n",
        "    # Option 2: In-memory (for development/testing)\n",
        "    # No setup needed - code automatically uses this if no server\n",
        "\n",
        "Architecture:\n",
        "    1. Extract 5 behavioral features from transaction\n",
        "    2. Create rich 12D embedding (features + account behavior + network stats)\n",
        "    3. XGBoost predicts base risk score\n",
        "    4. Qdrant finds similar historical fraud patterns\n",
        "    5. Combine scores: 70% XGBoost + 30% Similarity = Final Score\n",
        "    6. Export XGBoost model to ONNX format for AI agent deployment\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n",
        "                            roc_auc_score, roc_curve, precision_recall_curve, auc)\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue, SearchRequest\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ONNX export imports\n",
        "try:\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "    from skl2onnx import convert_sklearn\n",
        "    from skl2onnx.common.data_types import FloatTensorType\n",
        "    ONNX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ONNX_AVAILABLE = False\n",
        "    print(\"âš ï¸  ONNX libraries not available. Install with: pip install onnx onnxruntime skl2onnx\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "FRAUD_SEVERITY = {\n",
        "    'ACCOUNT_TAKEOVER': 95,\n",
        "    'RING_ACTIVITY': 90,\n",
        "    'LAYERING': 85,\n",
        "    'STRUCTURING': 85,\n",
        "    'MULE_OUT': 80,\n",
        "    'MULE_IN': 75,\n",
        "    'SCATTER': 70,\n",
        "    'GATHER': 70,\n",
        "    'TRANSFER': 0,\n",
        "}\n",
        "\n",
        "# Hybrid model weights\n",
        "XGBOOST_WEIGHT = 0.70  # 70% weight to XGBoost\n",
        "QDRANT_WEIGHT = 0.30   # 30% weight to Qdrant similarity\n",
        "\n",
        "# Drive path for saving models (modify this to your mounted drive path)\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/financialFraudDetectionUsingAIAgent\"\n",
        "\n",
        "# ============================================================================\n",
        "# QDRANT VECTOR STORE WITH RICH EMBEDDINGS\n",
        "# ============================================================================\n",
        "class FraudVectorStore:\n",
        "    \"\"\"\n",
        "    Qdrant vector store with RICH embeddings for fraud detection\n",
        "\n",
        "    Embedding Structure (12 dimensions):\n",
        "        [0-4]  : 5 core features (geo_distance, impossible_travel, etc.)\n",
        "        [5-7]  : Account behavior (age, transaction frequency, amount pattern)\n",
        "        [8-10] : Network features (unique recipients, device diversity, IP diversity)\n",
        "        [11]   : Time pattern (hour normalized)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, collection_name=\"fraud_embeddings\", use_memory=True):\n",
        "        \"\"\"Initialize Qdrant client\"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        # Try server, fallback to memory\n",
        "        if not use_memory:\n",
        "            try:\n",
        "                test_client = QdrantClient(host=\"localhost\", port=6333, timeout=2)\n",
        "                test_client.get_collections()\n",
        "                self.client = test_client\n",
        "                print(\"âœ“ Connected to Qdrant server at localhost:6333\")\n",
        "                return\n",
        "            except:\n",
        "                print(\"âš ï¸  Qdrant server not found, using in-memory mode\")\n",
        "\n",
        "        self.client = QdrantClient(\":memory:\")\n",
        "        print(\"âœ“ Using Qdrant in-memory mode\")\n",
        "\n",
        "    def create_rich_embeddings(self, df, features_df):\n",
        "        \"\"\"\n",
        "        Create 12D embeddings combining features + behavioral signals\n",
        "\n",
        "        Args:\n",
        "            df: Original dataframe with all transaction data\n",
        "            features_df: DataFrame with 5 core features\n",
        "\n",
        "        Returns:\n",
        "            12D embeddings (numpy array)\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        # Core 5 features (dimensions 0-4)\n",
        "        core_features = features_df.values\n",
        "\n",
        "        # Calculate additional behavioral features\n",
        "        # Account behavior (dimensions 5-7)\n",
        "        account_age_days = (df['timestamp'] - df.groupby('from_account')['timestamp'].transform('min')).dt.total_seconds() / 86400\n",
        "        account_age_normalized = np.clip(account_age_days / 365, 0, 1)  # Normalize to 0-1\n",
        "\n",
        "        account_tx_frequency = df.groupby('from_account').cumcount() + 1\n",
        "        tx_freq_normalized = np.clip(account_tx_frequency / 100, 0, 1)  # Normalize\n",
        "\n",
        "        amount_normalized = np.clip(df['amount'] / 10000, 0, 1)  # Normalize amounts\n",
        "\n",
        "        # Network features (dimensions 8-10)\n",
        "        unique_recipients = df.groupby('from_account')['to_account'].transform('nunique')\n",
        "        recipients_normalized = np.clip(unique_recipients / 50, 0, 1)\n",
        "\n",
        "        device_diversity = df.groupby('from_account')['device_id'].transform('nunique')\n",
        "        device_normalized = np.clip(device_diversity / 10, 0, 1)\n",
        "\n",
        "        if 'ip_address' in df.columns:\n",
        "            ip_diversity = df.groupby('from_account')['ip_address'].transform('nunique')\n",
        "            ip_normalized = np.clip(ip_diversity / 10, 0, 1)\n",
        "        else:\n",
        "            ip_normalized = np.zeros(len(df))\n",
        "\n",
        "        # Time pattern (dimension 11)\n",
        "        hour_normalized = df['timestamp'].dt.hour / 24\n",
        "\n",
        "        # Combine all features into 12D embeddings\n",
        "        behavioral_features = np.column_stack([\n",
        "            account_age_normalized,\n",
        "            tx_freq_normalized,\n",
        "            amount_normalized,\n",
        "            recipients_normalized,\n",
        "            device_normalized,\n",
        "            ip_normalized,\n",
        "            hour_normalized\n",
        "        ])\n",
        "\n",
        "        # Concatenate: [5 core features] + [7 behavioral features] = 12D\n",
        "        full_embeddings = np.hstack([core_features, behavioral_features])\n",
        "\n",
        "        # Normalize for cosine similarity\n",
        "        normalized_embeddings = self.scaler.fit_transform(full_embeddings)\n",
        "\n",
        "        print(f\"âœ“ Created {normalized_embeddings.shape[0]:,} embeddings of {normalized_embeddings.shape[1]}D\")\n",
        "\n",
        "        return normalized_embeddings\n",
        "\n",
        "    def create_collection(self, vector_size=12):\n",
        "        \"\"\"Create Qdrant collection\"\"\"\n",
        "        try:\n",
        "            self.client.delete_collection(collection_name=self.collection_name)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        self.client.create_collection(\n",
        "            collection_name=self.collection_name,\n",
        "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
        "        )\n",
        "        print(f\"âœ“ Created collection: {self.collection_name} ({vector_size}D vectors)\")\n",
        "\n",
        "    def add_transactions(self, embeddings, df, fraud_types, risk_scores, transaction_ids):\n",
        "        \"\"\"Add transactions with rich metadata\"\"\"\n",
        "\n",
        "        print(f\"Uploading {len(embeddings):,} vectors to Qdrant...\")\n",
        "\n",
        "        batch_size = 1000\n",
        "        total_batches = (len(embeddings) + batch_size - 1) // batch_size\n",
        "\n",
        "        for batch_idx in range(0, len(embeddings), batch_size):\n",
        "            batch_end = min(batch_idx + batch_size, len(embeddings))\n",
        "\n",
        "            points = []\n",
        "            for idx in range(batch_idx, batch_end):\n",
        "                # Rich payload for AI agent queries\n",
        "                payload = {\n",
        "                    \"transaction_id\": str(transaction_ids[idx]),\n",
        "                    \"fraud_type\": str(fraud_types.iloc[idx]),\n",
        "                    \"risk_score\": float(risk_scores.iloc[idx]),\n",
        "                    \"is_fraud\": int(risk_scores.iloc[idx] > 0),\n",
        "\n",
        "                    # Transaction details\n",
        "                    \"from_account\": str(df.iloc[idx]['from_account']),\n",
        "                    \"to_account\": str(df.iloc[idx]['to_account']),\n",
        "                    \"amount\": float(df.iloc[idx]['amount']),\n",
        "                    \"timestamp\": str(df.iloc[idx]['timestamp']),\n",
        "\n",
        "                    # Location\n",
        "                    \"country\": str(df.iloc[idx].get('location.country', 'Unknown')),\n",
        "                    \"city\": str(df.iloc[idx].get('location.city', 'Unknown')),\n",
        "\n",
        "                    # Device/IP\n",
        "                    \"device_id\": str(df.iloc[idx].get('device_id', 'Unknown')),\n",
        "                    \"ip_address\": str(df.iloc[idx].get('ip_address', 'Unknown')),\n",
        "\n",
        "                    # Features (for analysis)\n",
        "                    \"geo_distance\": float(df.iloc[idx]['geo_distance_km']),\n",
        "                    \"impossible_travel\": int(df.iloc[idx]['impossible_travel']),\n",
        "                    \"location_changes\": int(df.iloc[idx]['location_changes']),\n",
        "                    \"is_high_amount\": int(df.iloc[idx]['is_high_amount']),\n",
        "                    \"is_round_amount\": int(df.iloc[idx]['is_round_amount']),\n",
        "                }\n",
        "\n",
        "                point = PointStruct(\n",
        "                    id=idx,\n",
        "                    vector=embeddings[idx].tolist(),\n",
        "                    payload=payload\n",
        "                )\n",
        "                points.append(point)\n",
        "\n",
        "            self.client.upsert(collection_name=self.collection_name, points=points)\n",
        "\n",
        "            if (batch_idx // batch_size + 1) % 10 == 0 or batch_end == len(embeddings):\n",
        "                print(f\"  Progress: {batch_end:,}/{len(embeddings):,} ({batch_end/len(embeddings)*100:.1f}%)\")\n",
        "\n",
        "        print(f\"âœ“ Uploaded complete!\")\n",
        "\n",
        "    def get_similarity_score_batch(self, query_embeddings, top_k=20, fraud_only=True):\n",
        "        \"\"\"\n",
        "        Get similarity scores for multiple transactions at once (MUCH FASTER)\n",
        "\n",
        "        Args:\n",
        "            query_embeddings: numpy array of shape (n_samples, embedding_dim)\n",
        "            top_k: number of similar transactions to find\n",
        "            fraud_only: only search fraud transactions\n",
        "\n",
        "        Returns:\n",
        "            similarity_scores: array of scores (0-100)\n",
        "        \"\"\"\n",
        "\n",
        "        # Transform all embeddings at once\n",
        "        query_vectors = self.scaler.transform(query_embeddings).tolist()\n",
        "\n",
        "        # Build filter\n",
        "        if fraud_only:\n",
        "            search_filter = Filter(\n",
        "                must=[FieldCondition(key=\"is_fraud\", match=MatchValue(value=1))]\n",
        "            )\n",
        "        else:\n",
        "            search_filter = None\n",
        "\n",
        "        similarity_scores = []\n",
        "\n",
        "        # Batch search (process multiple at once)\n",
        "        batch_size = 100\n",
        "        for i in range(0, len(query_vectors), batch_size):\n",
        "            batch_vectors = query_vectors[i:i+batch_size]\n",
        "\n",
        "            for query_vector in batch_vectors:\n",
        "                try:\n",
        "                    results = self.client.search(\n",
        "                        collection_name=self.collection_name,\n",
        "                        query_vector=query_vector,\n",
        "                        limit=top_k,\n",
        "                        query_filter=search_filter\n",
        "                    )\n",
        "                except (AttributeError, TypeError):\n",
        "                    try:\n",
        "                        results = self.client.search(\n",
        "                            collection_name=self.collection_name,\n",
        "                            query_vector=query_vector,\n",
        "                            limit=top_k,\n",
        "                            search_filter=search_filter\n",
        "                        )\n",
        "                    except:\n",
        "                        from qdrant_client.http import models\n",
        "                        search_result = self.client.query_points(\n",
        "                            collection_name=self.collection_name,\n",
        "                            query=query_vector,\n",
        "                            limit=top_k,\n",
        "                            query_filter=search_filter,\n",
        "                            with_payload=True\n",
        "                        )\n",
        "                        results = search_result.points if hasattr(search_result, 'points') else []\n",
        "\n",
        "                if results:\n",
        "                    # Calculate weighted similarity score\n",
        "                    sim_scores = [r.score for r in results]\n",
        "                    fraud_risks = [r.payload['risk_score'] for r in results]\n",
        "                    weighted_risk = sum(s * r for s, r in zip(sim_scores, fraud_risks))\n",
        "                    total_weight = sum(sim_scores)\n",
        "                    score = weighted_risk / total_weight if total_weight > 0 else 0\n",
        "                    similarity_scores.append(score)\n",
        "                else:\n",
        "                    similarity_scores.append(0)\n",
        "\n",
        "            # Progress indicator\n",
        "            if (i + batch_size) % 1000 == 0:\n",
        "                print(f\"  Batch similarity progress: {i + batch_size}/{len(query_vectors)}\")\n",
        "\n",
        "        return np.array(similarity_scores)\n",
        "\n",
        "    def save_scaler(self, filepath):\n",
        "        \"\"\"Save the StandardScaler for later use\"\"\"\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        print(f\"âœ“ Saved scaler to: {filepath}\")\n",
        "\n",
        "# ============================================================================\n",
        "# HYBRID MODEL: XGBoost + Qdrant (OPTIMIZED)\n",
        "# ============================================================================\n",
        "class HybridFraudDetector:\n",
        "    \"\"\"\n",
        "    Hybrid model combining XGBoost and Qdrant similarity\n",
        "\n",
        "    Score = 0.7 * XGBoost + 0.3 * Qdrant Similarity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, xgboost_model, qdrant_store, xgb_weight=0.7, qdrant_weight=0.3):\n",
        "        self.xgboost = xgboost_model\n",
        "        self.qdrant = qdrant_store\n",
        "        self.xgb_weight = xgb_weight\n",
        "        self.qdrant_weight = qdrant_weight\n",
        "\n",
        "    def predict_batch(self, features_5d, embeddings_12d):\n",
        "        \"\"\"\n",
        "        Predict risk scores for multiple transactions at once (FAST)\n",
        "\n",
        "        Args:\n",
        "            features_5d: numpy array (n_samples, 5)\n",
        "            embeddings_12d: numpy array (n_samples, 12)\n",
        "\n",
        "        Returns:\n",
        "            hybrid_scores: array of combined risk scores\n",
        "            xgb_scores: array of XGBoost predictions\n",
        "            similarity_scores: array of Qdrant similarity scores\n",
        "        \"\"\"\n",
        "\n",
        "        # XGBoost predictions (vectorized - FAST)\n",
        "        xgb_scores = np.clip(self.xgboost.predict(features_5d), 0, 100)\n",
        "\n",
        "        # Qdrant similarity (batch processing - MUCH FASTER)\n",
        "        similarity_scores = self.qdrant.get_similarity_score_batch(\n",
        "            embeddings_12d, top_k=20, fraud_only=True\n",
        "        )\n",
        "\n",
        "        # Combine scores\n",
        "        hybrid_scores = (self.xgb_weight * xgb_scores +\n",
        "                        self.qdrant_weight * similarity_scores)\n",
        "\n",
        "        return hybrid_scores, xgb_scores, similarity_scores\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE ENGINEERING (Same as before)\n",
        "# ============================================================================\n",
        "def create_5_features(df):\n",
        "    \"\"\"Create 5 core features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Remove type column\n",
        "    fraud_types = None\n",
        "    if 'type' in df.columns:\n",
        "        fraud_types = df['type'].copy()\n",
        "        df = df.drop(columns=['type'])\n",
        "\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values(['from_account', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "    # 1. geo_distance_km\n",
        "    if all(col in df.columns for col in ['location.latitude', 'location.longitude']):\n",
        "        df['prev_lat'] = df.groupby('from_account')['location.latitude'].shift(1)\n",
        "        df['prev_lon'] = df.groupby('from_account')['location.longitude'].shift(1)\n",
        "        df['prev_lat'] = df['prev_lat'].fillna(df['location.latitude'])\n",
        "        df['prev_lon'] = df['prev_lon'].fillna(df['location.longitude'])\n",
        "\n",
        "        lat1 = np.radians(df['prev_lat'])\n",
        "        lon1 = np.radians(df['prev_lon'])\n",
        "        lat2 = np.radians(df['location.latitude'])\n",
        "        lon2 = np.radians(df['location.longitude'])\n",
        "\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
        "        df['geo_distance_km'] = 6371 * c\n",
        "    else:\n",
        "        df['geo_distance_km'] = 0\n",
        "\n",
        "    # 2. impossible_travel\n",
        "    df['prev_timestamp'] = df.groupby('from_account')['timestamp'].shift(1)\n",
        "    df['time_diff_hours'] = (df['timestamp'] - df['prev_timestamp']).dt.total_seconds() / 3600\n",
        "    df['time_diff_hours'] = df['time_diff_hours'].fillna(24).replace(0, 0.001)\n",
        "    df['travel_speed_kmh'] = df['geo_distance_km'] / df['time_diff_hours']\n",
        "    df['impossible_travel'] = (df['travel_speed_kmh'] > 800).astype(int)\n",
        "\n",
        "    # 3. location_changes\n",
        "    df['location_moved'] = (df['geo_distance_km'] > 1).astype(int)\n",
        "    df['location_changes'] = df.groupby('from_account')['location_moved'].cumsum()\n",
        "\n",
        "    # 4. is_high_amount\n",
        "    threshold_95 = df['amount'].quantile(0.95)\n",
        "    df['is_high_amount'] = (df['amount'] >= threshold_95).astype(int)\n",
        "\n",
        "    # 5. is_round_amount\n",
        "    df['is_round_amount'] = (df['amount'] % 100 == 0).astype(int)\n",
        "\n",
        "    return df, fraud_types\n",
        "\n",
        "# ============================================================================\n",
        "# ONNX EXPORT FUNCTIONS\n",
        "# ============================================================================\n",
        "def export_xgboost_to_onnx(xgb_model, output_path, feature_names, n_features=5):\n",
        "    \"\"\"\n",
        "    Export XGBoost model to ONNX format\n",
        "\n",
        "    Args:\n",
        "        xgb_model: Trained XGBoost model\n",
        "        output_path: Path to save ONNX model\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of input features\n",
        "    \"\"\"\n",
        "    if not ONNX_AVAILABLE:\n",
        "        print(\"âŒ ONNX libraries not available. Cannot export model.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"EXPORTING MODEL TO ONNX FORMAT\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        # Define the input type for ONNX\n",
        "        initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
        "\n",
        "        # Convert to ONNX\n",
        "        print(f\"\\n[1/3] Converting XGBoost model to ONNX...\")\n",
        "        onnx_model = convert_sklearn(\n",
        "            xgb_model,\n",
        "            initial_types=initial_type,\n",
        "            target_opset=12,\n",
        "            options={id(xgb_model): {'zipmap': False}}\n",
        "        )\n",
        "\n",
        "        # Save ONNX model\n",
        "        print(f\"[2/3] Saving ONNX model to: {output_path}\")\n",
        "        with open(output_path, \"wb\") as f:\n",
        "            f.write(onnx_model.SerializeToString())\n",
        "\n",
        "        # Verify the model\n",
        "        print(f\"[3/3] Verifying ONNX model...\")\n",
        "        onnx_model_check = onnx.load(output_path)\n",
        "        onnx.checker.check_model(onnx_model_check)\n",
        "\n",
        "        # Test inference\n",
        "        print(f\"\\n[TEST] Testing ONNX inference...\")\n",
        "        ort_session = ort.InferenceSession(output_path)\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = np.random.rand(1, n_features).astype(np.float32)\n",
        "        ort_inputs = {ort_session.get_inputs()[0].name: dummy_input}\n",
        "        ort_outputs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "        print(f\"âœ“ ONNX model successfully exported and verified!\")\n",
        "        print(f\"\\nModel Details:\")\n",
        "        print(f\"  - Input shape: (batch_size, {n_features})\")\n",
        "        print(f\"  - Output shape: (batch_size, 1)\")\n",
        "        print(f\"  - Features: {feature_names}\")\n",
        "        print(f\"  - File size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error exporting to ONNX: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def save_model_metadata(metadata_path, model_info):\n",
        "    \"\"\"Save model metadata as JSON\"\"\"\n",
        "    os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "    print(f\"âœ“ Saved model metadata to: {metadata_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING (WITH ONNX EXPORT)\n",
        "# ============================================================================\n",
        "def train_hybrid_model(json_file_path, save_path=DRIVE_PATH):\n",
        "    \"\"\"Train hybrid XGBoost + Qdrant model and export to ONNX\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"HYBRID FRAUD DETECTION: XGBoost + Qdrant Vector Similarity\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n[1/7] Loading data...\")\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    df_original = pd.json_normalize(data)\n",
        "    print(f\"Loaded {len(df_original):,} transactions\")\n",
        "\n",
        "    # Engineer features\n",
        "    print(\"\\n[2/7] Engineering 5 core features...\")\n",
        "    df, fraud_types = create_5_features(df_original.copy())\n",
        "\n",
        "    # Create target\n",
        "    print(\"\\n[3/7] Creating target...\")\n",
        "    y = fraud_types.map(FRAUD_SEVERITY).fillna(0)\n",
        "    print(f\"Fraud: {(y > 0).sum():,} ({(y > 0).mean()*100:.1f}%)\")\n",
        "\n",
        "    # Select 5D features for XGBoost\n",
        "    feature_names = ['geo_distance_km', 'impossible_travel', 'location_changes',\n",
        "                     'is_high_amount', 'is_round_amount']\n",
        "    X_5d = df[feature_names].copy()\n",
        "    X_5d = X_5d.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    print(f\"\\nFeature matrix: {X_5d.shape}\")\n",
        "\n",
        "    # Split data\n",
        "    print(\"\\n[4/7] Splitting data...\")\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X_5d, y, test_size=0.3, random_state=42, stratify=(y > 0)\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=(y_temp > 0)\n",
        "    )\n",
        "\n",
        "    train_indices = X_train.index\n",
        "    val_indices = X_val.index\n",
        "    test_indices = X_test.index\n",
        "\n",
        "    print(f\"Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\")\n",
        "\n",
        "    # Train XGBoost\n",
        "    print(\"\\n[5/7] Training XGBoost...\")\n",
        "    xgb_model = XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=50)\n",
        "\n",
        "    print(\"\\n[6/7] Creating Qdrant vector store...\")\n",
        "    qdrant = FraudVectorStore(use_memory=True)\n",
        "    qdrant.create_collection(vector_size=12)\n",
        "\n",
        "    # Create 12D embeddings for TRAINING data\n",
        "    print(\"\\nCreating embeddings for training data...\")\n",
        "    train_embeddings = qdrant.create_rich_embeddings(\n",
        "        df.iloc[train_indices],\n",
        "        X_train\n",
        "    )\n",
        "\n",
        "    # Add training data to Qdrant\n",
        "    qdrant.add_transactions(\n",
        "        embeddings=train_embeddings,\n",
        "        df=df.iloc[train_indices],\n",
        "        fraud_types=fraud_types.iloc[train_indices],\n",
        "        risk_scores=y_train,\n",
        "        transaction_ids=train_indices\n",
        "    )\n",
        "\n",
        "    print(\"\\n[7/7] Generating hybrid predictions on test set...\")\n",
        "\n",
        "    # Create 12D embeddings for TEST data\n",
        "    test_embeddings = qdrant.create_rich_embeddings(\n",
        "        df.iloc[test_indices],\n",
        "        X_test\n",
        "    )\n",
        "\n",
        "    # Create hybrid model\n",
        "    hybrid_model = HybridFraudDetector(\n",
        "        xgboost_model=xgb_model,\n",
        "        qdrant_store=qdrant,\n",
        "        xgb_weight=XGBOOST_WEIGHT,\n",
        "        qdrant_weight=QDRANT_WEIGHT\n",
        "    )\n",
        "\n",
        "    # Generate hybrid predictions\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    y_test_hybrid, y_test_xgb, y_test_similarity = hybrid_model.predict_batch(\n",
        "        X_test.values,\n",
        "        test_embeddings\n",
        "    )\n",
        "\n",
        "    # ========================================================================\n",
        "    # EVALUATION\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    y_test_binary = (y_test > 0).astype(int)\n",
        "\n",
        "    # XGBoost only\n",
        "    auc_xgb = roc_auc_score(y_test_binary, y_test_xgb)\n",
        "    precision_xgb, recall_xgb, _ = precision_recall_curve(y_test_binary, y_test_xgb)\n",
        "    auc_pr_xgb = auc(recall_xgb, precision_xgb)\n",
        "\n",
        "    # Hybrid\n",
        "    auc_hybrid = roc_auc_score(y_test_binary, y_test_hybrid)\n",
        "    precision_hybrid, recall_hybrid, _ = precision_recall_curve(y_test_binary, y_test_hybrid)\n",
        "    auc_pr_hybrid = auc(recall_hybrid, precision_hybrid)\n",
        "\n",
        "    print(f\"\\nXGBoost Only:\")\n",
        "    print(f\"  AUC-ROC: {auc_xgb:.4f}\")\n",
        "    print(f\"  AUC-PR:  {auc_pr_xgb:.4f}\")\n",
        "\n",
        "    print(f\"\\nHybrid (XGBoost + Qdrant):\")\n",
        "    print(f\"  AUC-ROC: {auc_hybrid:.4f}  {'âœ… +' + f'{(auc_hybrid-auc_xgb):.4f}' if auc_hybrid > auc_xgb else 'âš ï¸  ' + f'{(auc_hybrid-auc_xgb):.4f}'}\")\n",
        "    print(f\"  AUC-PR:  {auc_pr_hybrid:.4f}  {'âœ… +' + f'{(auc_pr_hybrid-auc_pr_xgb):.4f}' if auc_pr_hybrid > auc_pr_xgb else 'âš ï¸  ' + f'{(auc_pr_hybrid-auc_pr_xgb):.4f}'}\")\n",
        "\n",
        "    improvement = ((auc_hybrid - auc_xgb) / auc_xgb * 100) if auc_xgb > 0 else 0\n",
        "    print(f\"\\nâœ¨ Qdrant improves AUC-ROC by {improvement:.2f}%\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE MODELS\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING MODELS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # 1. Save XGBoost model as ONNX\n",
        "    onnx_path = os.path.join(save_path, f\"fraud_model_xgboost_{timestamp}.onnx\")\n",
        "    export_success = export_xgboost_to_onnx(\n",
        "        xgb_model,\n",
        "        onnx_path,\n",
        "        feature_names,\n",
        "        n_features=len(feature_names)\n",
        "    )\n",
        "\n",
        "    # 2. Save XGBoost model in native format (backup)\n",
        "    xgb_native_path = os.path.join(save_path, f\"fraud_model_xgboost_{timestamp}.json\")\n",
        "    xgb_model.save_model(xgb_native_path)\n",
        "    print(f\"\\nâœ“ Saved XGBoost native model to: {xgb_native_path}\")\n",
        "\n",
        "    # 3. Save Qdrant scaler\n",
        "    scaler_path = os.path.join(save_path, f\"qdrant_scaler_{timestamp}.pkl\")\n",
        "    qdrant.save_scaler(scaler_path)\n",
        "\n",
        "    # 4. Save model metadata\n",
        "    metadata = {\n",
        "        \"model_type\": \"Hybrid XGBoost + Qdrant\",\n",
        "        \"timestamp\": timestamp,\n",
        "        \"training_date\": datetime.now().isoformat(),\n",
        "        \"feature_names\": feature_names,\n",
        "        \"n_features\": len(feature_names),\n",
        "        \"embedding_dimension\": 12,\n",
        "        \"xgboost_weight\": XGBOOST_WEIGHT,\n",
        "        \"qdrant_weight\": QDRANT_WEIGHT,\n",
        "        \"performance\": {\n",
        "            \"xgboost_auc_roc\": float(auc_xgb),\n",
        "            \"xgboost_auc_pr\": float(auc_pr_xgb),\n",
        "            \"hybrid_auc_roc\": float(auc_hybrid),\n",
        "            \"hybrid_auc_pr\": float(auc_pr_hybrid),\n",
        "            \"improvement_percent\": float(improvement)\n",
        "        },\n",
        "        \"training_data\": {\n",
        "            \"total_samples\": len(df_original),\n",
        "            \"fraud_samples\": int((y > 0).sum()),\n",
        "            \"fraud_percentage\": float((y > 0).mean() * 100),\n",
        "            \"train_size\": len(X_train),\n",
        "            \"val_size\": len(X_val),\n",
        "            \"test_size\": len(X_test)\n",
        "        },\n",
        "        \"files\": {\n",
        "            \"onnx_model\": onnx_path,\n",
        "            \"xgboost_native\": xgb_native_path,\n",
        "            \"scaler\": scaler_path,\n",
        "        },\n",
        "        \"qdrant_config\": {\n",
        "            \"collection_name\": qdrant.collection_name,\n",
        "            \"vector_dimension\": 12,\n",
        "            \"distance_metric\": \"cosine\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(save_path, f\"model_metadata_{timestamp}.json\")\n",
        "    save_model_metadata(metadata_path, metadata)\n",
        "\n",
        "    return hybrid_model, qdrant, X_test, y_test, y_test_hybrid, auc_xgb, auc_hybrid, metadata\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Update this path to your actual JSON file location\n",
        "    json_file = os.path.join(DRIVE_PATH, \"fraud_data.json\")\n",
        "\n",
        "    model, qdrant, X_test, y_test, predictions, auc_xgb, auc_hybrid, metadata = train_hybrid_model(\n",
        "        json_file,\n",
        "        save_path=DRIVE_PATH\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âœ… HYBRID MODEL TRAINING & EXPORT COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"  XGBoost AUC-ROC: {auc_xgb:.4f}\")\n",
        "    print(f\"  Hybrid AUC-ROC:  {auc_hybrid:.4f}\")\n",
        "    print(f\"  Improvement:     +{((auc_hybrid-auc_xgb)/auc_xgb*100):.2f}%\")\n",
        "\n",
        "    print(\"\\nğŸ“ Saved Files:\")\n",
        "    print(f\"  - ONNX Model: {metadata['files']['onnx_model']}\")\n",
        "    print(f\"  - XGBoost Native: {metadata['files']['xgboost_native']}\")\n",
        "    print(f\"  - Scaler: {metadata['files']['scaler']}\")\n",
        "    print(f\"  - Metadata: {metadata['files'].get('metadata', 'N/A')}\")\n",
        "\n",
        "    print(\"\\nğŸ¤– For AI Agent Integration:\")\n",
        "    print(\"  1. Use ONNX model for inference (fastest)\")\n",
        "    print(\"  2. Load scaler for embedding normalization\")\n",
        "    print(\"  3. Connect to Qdrant collection: 'fraud_embeddings'\")\n",
        "    print(\"  4. Input: 5D feature vector\")\n",
        "    print(\"  5. Output: Risk score (0-100)\")\n",
        "\n",
        "    print(\"\\nğŸ“š ONNX Model Usage Example:\")\n",
        "    print(\"```python\")\n",
        "    print(\"import onnxruntime as ort\")\n",
        "    print(\"import numpy as np\")\n",
        "    print(\"\")\n",
        "    print(\"# Load ONNX model\")\n",
        "    print(f\"session = ort.InferenceSession('{os.path.basename(metadata['files']['onnx_model'])}')\")\n",
        "    print(\"\")\n",
        "    print(\"# Prepare input (5 features)\")\n",
        "    print(\"input_data = np.array([[geo_distance, impossible_travel, location_changes,\")\n",
        "    print(\"                        is_high_amount, is_round_amount]], dtype=np.float32)\")\n",
        "    print(\"\")\n",
        "    print(\"# Run inference\")\n",
        "    print(\"input_name = session.get_inputs()[0].name\")\n",
        "    print(\"output = session.run(None, {input_name: input_data})\")\n",
        "    print(\"risk_score = output[0][0]\")\n",
        "    print(\"```\")\n",
        "# fraud_detection_with_onnx_export.py\n",
        "# Displaying fraud_detection_with_onnx_export.py."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egpuew7VlzbV",
        "outputId": "1586da69-45dc-4fc9-d413-dd88bec7d8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸  ONNX libraries not available. Install with: pip install onnx onnxruntime skl2onnx\n",
            "================================================================================\n",
            "HYBRID FRAUD DETECTION: XGBoost + Qdrant Vector Similarity\n",
            "================================================================================\n",
            "\n",
            "[1/7] Loading data...\n",
            "Loaded 100,000 transactions\n",
            "\n",
            "[2/7] Engineering 5 core features...\n",
            "\n",
            "[3/7] Creating target...\n",
            "Fraud: 49,243 (49.2%)\n",
            "\n",
            "Feature matrix: (100000, 5)\n",
            "\n",
            "[4/7] Splitting data...\n",
            "Train: 70,000 | Val: 15,000 | Test: 15,000\n",
            "\n",
            "[5/7] Training XGBoost...\n",
            "[0]\tvalidation_0-rmse:41.03182\n",
            "[50]\tvalidation_0-rmse:40.99818\n",
            "[100]\tvalidation_0-rmse:41.00202\n",
            "[150]\tvalidation_0-rmse:40.99097\n",
            "[199]\tvalidation_0-rmse:40.98784\n",
            "\n",
            "[6/7] Creating Qdrant vector store...\n",
            "âœ“ Using Qdrant in-memory mode\n",
            "âœ“ Created collection: fraud_embeddings (12D vectors)\n",
            "\n",
            "Creating embeddings for training data...\n",
            "âœ“ Created 70,000 embeddings of 12D\n",
            "Uploading 70,000 vectors to Qdrant...\n",
            "  Progress: 10,000/70,000 (14.3%)\n",
            "  Progress: 20,000/70,000 (28.6%)\n",
            "  Progress: 30,000/70,000 (42.9%)\n",
            "  Progress: 40,000/70,000 (57.1%)\n",
            "  Progress: 50,000/70,000 (71.4%)\n",
            "  Progress: 60,000/70,000 (85.7%)\n",
            "  Progress: 70,000/70,000 (100.0%)\n",
            "âœ“ Uploaded complete!\n",
            "\n",
            "[7/7] Generating hybrid predictions on test set...\n",
            "âœ“ Created 15,000 embeddings of 12D\n",
            "\n",
            "Generating predictions...\n",
            "  Batch similarity progress: 1000/15000\n",
            "  Batch similarity progress: 2000/15000\n",
            "  Batch similarity progress: 3000/15000\n",
            "  Batch similarity progress: 4000/15000\n",
            "  Batch similarity progress: 5000/15000\n",
            "  Batch similarity progress: 6000/15000\n",
            "  Batch similarity progress: 7000/15000\n",
            "  Batch similarity progress: 8000/15000\n",
            "  Batch similarity progress: 9000/15000\n",
            "  Batch similarity progress: 10000/15000\n",
            "  Batch similarity progress: 11000/15000\n",
            "  Batch similarity progress: 12000/15000\n",
            "  Batch similarity progress: 13000/15000\n",
            "  Batch similarity progress: 14000/15000\n",
            "  Batch similarity progress: 15000/15000\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON\n",
            "================================================================================\n",
            "\n",
            "XGBoost Only:\n",
            "  AUC-ROC: 0.5364\n",
            "  AUC-PR:  0.5267\n",
            "\n",
            "Hybrid (XGBoost + Qdrant):\n",
            "  AUC-ROC: 0.5347  âš ï¸  -0.0017\n",
            "  AUC-PR:  0.5262  âš ï¸  -0.0004\n",
            "\n",
            "âœ¨ Qdrant improves AUC-ROC by -0.32%\n",
            "\n",
            "================================================================================\n",
            "SAVING MODELS\n",
            "================================================================================\n",
            "âŒ ONNX libraries not available. Cannot export model.\n",
            "\n",
            "âœ“ Saved XGBoost native model to: /content/drive/MyDrive/financialFraudDetectionUsingAIAgent/fraud_model_xgboost_20260130_080815.json\n",
            "âœ“ Saved scaler to: /content/drive/MyDrive/financialFraudDetectionUsingAIAgent/qdrant_scaler_20260130_080815.pkl\n",
            "âœ“ Saved model metadata to: /content/drive/MyDrive/financialFraudDetectionUsingAIAgent/model_metadata_20260130_080815.json\n",
            "\n",
            "================================================================================\n",
            "âœ… HYBRID MODEL TRAINING & EXPORT COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Model Performance:\n",
            "  XGBoost AUC-ROC: 0.5364\n",
            "  Hybrid AUC-ROC:  0.5347\n",
            "  Improvement:     +-0.32%\n",
            "\n",
            "ğŸ“ Saved Files:\n",
            "  - ONNX Model: /content/drive/MyDrive/financialFraudDetectionUsingAIAgent/fraud_model_xgboost_20260130_080815.onnx\n",
            "  - XGBoost Native: /content/drive/MyDrive/financialFraudDetectionUsingAIAgent/fraud_model_xgboost_20260130_080815.json\n",
            "  - Scaler: /content/drive/MyDrive/financialFraudDetectionUsingAIAgent/qdrant_scaler_20260130_080815.pkl\n",
            "  - Metadata: N/A\n",
            "\n",
            "ğŸ¤– For AI Agent Integration:\n",
            "  1. Use ONNX model for inference (fastest)\n",
            "  2. Load scaler for embedding normalization\n",
            "  3. Connect to Qdrant collection: 'fraud_embeddings'\n",
            "  4. Input: 5D feature vector\n",
            "  5. Output: Risk score (0-100)\n",
            "\n",
            "ğŸ“š ONNX Model Usage Example:\n",
            "```python\n",
            "import onnxruntime as ort\n",
            "import numpy as np\n",
            "\n",
            "# Load ONNX model\n",
            "session = ort.InferenceSession('fraud_model_xgboost_20260130_080815.onnx')\n",
            "\n",
            "# Prepare input (5 features)\n",
            "input_data = np.array([[geo_distance, impossible_travel, location_changes,\n",
            "                        is_high_amount, is_round_amount]], dtype=np.float32)\n",
            "\n",
            "# Run inference\n",
            "input_name = session.get_inputs()[0].name\n",
            "output = session.run(None, {input_name: input_data})\n",
            "risk_score = output[0][0]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"âœ… GPU is available\")\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"âŒ GPU is NOT available (using CPU)\")\n"
      ],
      "metadata": {
        "id": "iA3YXdzZDZFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffcbc328-6c27-4e24-c149-a71e7d49841f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPU is available\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVWh0PdTh0PN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}